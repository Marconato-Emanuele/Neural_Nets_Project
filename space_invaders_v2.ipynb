{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDUuD7YFnx4s"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import imageio\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQzPxj0MpyWg"
   },
   "outputs": [],
   "source": [
    "STORE_PATH = 'Projetto_Reti2020'\n",
    "MAX_EPSILON = 0.9\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_MIN_ITER = 5000\n",
    "GAMMA = 0.99 #discount factor \n",
    "BATCH_SIZE = 32\n",
    "TAU = 0.08 \n",
    "DELAY_TRAINING = 25000 \n",
    "NUM_FRAMES = 4 #stacked frames to train the network\n",
    "GIF_RECORDING_FREQ = 100\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "num_actions = env.action_space.n\n",
    "space_dim   = env.observation_space.shape\n",
    "\n",
    "POST_PROCESS_IMAGE_SIZE = (105, 80, 4) #resize the image from (210,160) to (105,80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNdWr2TZqXkh"
   },
   "source": [
    "Definition of the model:\n",
    "* 3 convolutional layers (to extract relevant features)\n",
    "* 1 flatten (mute reshaping layer)\n",
    "* 1 fully-connected layer \n",
    "* 1 output layer with n=num_actions units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shaVe-T9xEFs"
   },
   "outputs": [],
   "source": [
    "def CNN(input_shape=(105,80,4), output_layer= 3, last_activation = None):\n",
    "    model = keras.Sequential(\n",
    "      [keras.Input(shape=input_shape, name=\"input_layer\" ),\n",
    "       layers.Conv2D( filters=16, kernel_size= 8,strides=4, padding=\"valid\", activation = \"relu\" ),\n",
    "       layers.Conv2D( filters=32, kernel_size= 4,strides=2, padding=\"valid\", activation = \"relu\" ),\n",
    "       layers.Conv2D( filters= 32, kernel_size= 3,strides=1, padding=\"valid\", activation = \"relu\" )             \n",
    "      ]\n",
    "    ) \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(units= output_layer, activation = last_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9H5m25mXqiDh"
   },
   "source": [
    "Network initialization. \\\n",
    "Note: target network is set as a copy of primary network.\\\n",
    "OPTIMIZER = Adam with loss mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVovX1rStX-x"
   },
   "outputs": [],
   "source": [
    "primary_network = CNN(input_shape= POST_PROCESS_IMAGE_SIZE, output_layer=num_actions)\n",
    "target_network  = CNN(input_shape= POST_PROCESS_IMAGE_SIZE, output_layer= num_actions)\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "# make target_network = primary_network\n",
    "for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "    t.assign(e)\n",
    "\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "NZHhumgPxTuE",
    "outputId": "7935fe6e-741f-4e34-abe5-76131ec2e72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 25, 19, 16)        4112      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 8, 32)         8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 6, 32)          9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1728)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               221312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 243,412\n",
      "Trainable params: 243,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "primary_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8q1RrhZHqozM"
   },
   "source": [
    "Memory class buffer, with add_sample to store transitions and sample to extract a batch of BATCH_SIZE dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sitNfMgqldY"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._actions = np.zeros(max_memory, dtype=np.int32)\n",
    "        self._rewards = np.zeros(max_memory, dtype=np.float32)\n",
    "        self._frames = np.zeros((POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], max_memory), dtype=np.float32)\n",
    "        self._terminal = np.zeros(max_memory, dtype=np.bool)\n",
    "        self._i = 0\n",
    "\n",
    "    def add_sample(self, frame, action, reward, terminal):\n",
    "        self._actions[self._i] = action\n",
    "        self._rewards[self._i] = reward\n",
    "        self._frames[:, :, self._i] = frame[:, :, 0]\n",
    "        self._terminal[self._i] = terminal\n",
    "        if self._i % (self._max_memory - 1) == 0 and self._i != 0:\n",
    "            self._i = BATCH_SIZE + NUM_FRAMES + 1\n",
    "        else:\n",
    "            self._i += 1\n",
    "\n",
    "    def sample(self):\n",
    "        if self._i < BATCH_SIZE + NUM_FRAMES + 1:\n",
    "            raise ValueError(\"Not enough memory to extract a batch\")\n",
    "        else:\n",
    "            rand_idxs = np.random.randint(NUM_FRAMES + 1, self._i, size=BATCH_SIZE)\n",
    "            states = np.zeros((BATCH_SIZE, POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES),\n",
    "                             dtype=np.float32)\n",
    "            next_states = np.zeros((BATCH_SIZE, POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES),\n",
    "                             dtype=np.float32)\n",
    "            for i, idx in enumerate(rand_idxs):\n",
    "                states[i] = self._frames[:, :, idx - 1 - NUM_FRAMES:idx - 1]\n",
    "                next_states[i] = self._frames[:, :, idx - NUM_FRAMES:idx]\n",
    "            return states, self._actions[rand_idxs], self._rewards[rand_idxs], next_states, self._terminal[rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTnaHEa9qzd_"
   },
   "outputs": [],
   "source": [
    "#start memory as large as possible (based on aviable RAM) to store more episodes\n",
    "memory = Memory(150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rb9XVfF9q-us"
   },
   "source": [
    "Some utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYRoaQJEq5xC"
   },
   "outputs": [],
   "source": [
    "def image_preprocess(image, new_size=(105,80)):\n",
    "    # convert to greyscale, resize and normalize the image\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image, new_size)\n",
    "    image = image / 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def choose_action(state, primary_network, eps, step):\n",
    "  #eps-greedy action after DELAY_TRAINING\n",
    "    if step < DELAY_TRAINING:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        if random.random() < eps:\n",
    "            return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(primary_network(tf.reshape(state, (1, POST_PROCESS_IMAGE_SIZE[0],\n",
    "                                                           POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES)).numpy()))\n",
    "\n",
    "\n",
    "def update_network(primary_network, target_network, check= False):\n",
    "    # update target network parameters slowly from primary network\n",
    "    #completely after N episodes\n",
    "    if check:\n",
    "        tau = 1\n",
    "    else: tau = 0.08\n",
    "    for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "        t.assign(t * (1 - tau) + e * tau)\n",
    "\n",
    "\n",
    "def process_state_stack(state_stack, state):\n",
    "  #images stack\n",
    "    for i in range(1, state_stack.shape[-1]):\n",
    "        state_stack[:, :, i - 1].assign(state_stack[:, :, i])\n",
    "    state_stack[:, :, -1].assign(state[:, :, 0])\n",
    "    return state_stack\n",
    "\n",
    "\n",
    "def record_gif(frame_list, episode, fps=50):\n",
    "    imageio.mimsave(STORE_PATH + f\"/SpaceInvaders_EPISODE-{episode}.gif\", frame_list, fps=fps) #duration=duration_per_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLDcUFPurGV4"
   },
   "source": [
    "Training structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSq-cjSXrB-n"
   },
   "outputs": [],
   "source": [
    "def train(primary_network, memory, target_network=None):\n",
    "    #Train function based on DQN with single step\n",
    "    states, actions, rewards, next_states, terminal = memory.sample()\n",
    "    # predict Q(s,a) given the batch of states\n",
    "    prim_qt = primary_network(states)\n",
    "    # predict Q(s',a') from the evaluation network\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "    target_q = prim_qt.numpy()\n",
    "    updates = rewards\n",
    "    valid_idxs = terminal != True\n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    if target_network is None:\n",
    "        updates[valid_idxs] += GAMMA * np.amax(prim_qtp1.numpy()[valid_idxs, :], axis=1) #never used this\n",
    "    else:\n",
    "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "        q_from_target = target_network(next_states)\n",
    "        updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "    target_q[batch_idxs, actions] = updates\n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOK271ha-U1b"
   },
   "source": [
    "GPU initialization in COLAB. This is not necessary if the compiler has no GPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P_jB4Ozmsq2b",
    "outputId": "b20c5214-eddb-4521-bb0b-08a99620268c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHEE56tUsyWA"
   },
   "outputs": [],
   "source": [
    "#set how many records are necessary before to start batch-sampling from the memory class \n",
    "DELAY_TRAINING = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "McUACNyXrKRC",
    "outputId": "13cc4b8c-e5dd-46bd-b561-69908abff265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training...Episode: 0\n",
      "Pre-training...Episode: 1\n",
      "Pre-training...Episode: 2\n",
      "Pre-training...Episode: 3\n",
      "Pre-training...Episode: 4\n",
      "Pre-training...Episode: 5\n",
      "Pre-training...Episode: 6\n",
      "Pre-training...Episode: 7\n",
      "Pre-training...Episode: 8\n",
      "Pre-training...Episode: 9\n",
      "Pre-training...Episode: 10\n",
      "Pre-training...Episode: 11\n",
      "Pre-training...Episode: 12\n",
      "Pre-training...Episode: 13\n",
      "Pre-training...Episode: 14\n",
      "Pre-training...Episode: 15\n",
      "Pre-training...Episode: 16\n",
      "Pre-training...Episode: 17\n",
      "Pre-training...Episode: 18\n",
      "Pre-training...Episode: 19\n",
      "Pre-training...Episode: 20\n",
      "Pre-training...Episode: 21\n",
      "Pre-training...Episode: 22\n",
      "Pre-training...Episode: 23\n",
      "Pre-training...Episode: 24\n",
      "Pre-training...Episode: 25\n",
      "Pre-training...Episode: 26\n",
      "Pre-training...Episode: 27\n",
      "Pre-training...Episode: 28\n",
      "Pre-training...Episode: 29\n",
      "Pre-training...Episode: 30\n",
      "Pre-training...Episode: 31\n",
      "Pre-training...Episode: 32\n",
      "Pre-training...Episode: 33\n",
      "Pre-training...Episode: 34\n",
      "Pre-training...Episode: 35\n",
      "Pre-training...Episode: 36\n",
      "Pre-training...Episode: 37\n",
      "Pre-training...Episode: 38\n",
      "Pre-training...Episode: 39\n",
      "Pre-training...Episode: 40\n",
      "Pre-training...Episode: 41\n",
      "Pre-training...Episode: 42\n",
      "Pre-training...Episode: 43\n",
      "Pre-training...Episode: 44\n",
      "Pre-training...Episode: 45\n",
      "Pre-training...Episode: 46\n",
      "Pre-training...Episode: 47\n",
      "Pre-training...Episode: 48\n",
      "Pre-training...Episode: 49\n",
      "Pre-training...Episode: 50\n",
      "Pre-training...Episode: 51\n",
      "Pre-training...Episode: 52\n",
      "Pre-training...Episode: 53\n",
      "Pre-training...Episode: 54\n",
      "Pre-training...Episode: 55\n",
      "Pre-training...Episode: 56\n",
      "Pre-training...Episode: 57\n",
      "Pre-training...Episode: 58\n",
      "Pre-training...Episode: 59\n",
      "Pre-training...Episode: 60\n",
      "Pre-training...Episode: 61\n",
      "Pre-training...Episode: 62\n",
      "Pre-training...Episode: 63\n",
      "Pre-training...Episode: 64\n",
      "Pre-training...Episode: 65\n",
      "Pre-training...Episode: 66\n",
      "Pre-training...Episode: 67\n",
      "Pre-training...Episode: 68\n",
      "Pre-training...Episode: 69\n",
      "Pre-training...Episode: 70\n",
      "Pre-training...Episode: 71\n",
      "Pre-training...Episode: 72\n",
      "Pre-training...Episode: 73\n",
      "Pre-training...Episode: 74\n",
      "Pre-training...Episode: 75\n",
      "Pre-training...Episode: 76\n",
      "Pre-training...Episode: 77\n",
      "Pre-training...Episode: 78\n",
      "Pre-training...Episode: 79\n",
      "Pre-training...Episode: 80\n",
      "Pre-training...Episode: 81\n",
      "Pre-training...Episode: 82\n",
      "Pre-training...Episode: 83\n",
      "Pre-training...Episode: 84\n",
      "Pre-training...Episode: 85\n",
      "Pre-training...Episode: 86\n",
      "Pre-training...Episode: 87\n",
      "Pre-training...Episode: 88\n",
      "Pre-training...Episode: 89\n",
      "Pre-training...Episode: 90\n",
      "Pre-training...Episode: 91\n",
      "Pre-training...Episode: 92\n",
      "Pre-training...Episode: 93\n",
      "Pre-training...Episode: 94\n",
      "Pre-training...Episode: 95\n",
      "Pre-training...Episode: 96\n",
      "Pre-training...Episode: 97\n",
      "Pre-training...Episode: 98\n",
      "Episode: 99, Reward: 0.0, avg loss: -0.25214, eps: 0.600, Time:  7\n",
      "Episode: 100, Reward: 0.0, avg loss: 0.00058, eps: 0.600, Time:  10\n",
      "Episode: 101, Reward: 1.0, avg loss: 0.00068, eps: 0.599, Time:  11\n",
      "Episode: 102, Reward: 0.0, avg loss: 0.00063, eps: 0.599, Time:  8\n",
      "Episode: 103, Reward: 2.0, avg loss: 0.00061, eps: 0.599, Time:  15\n",
      "Episode: 104, Reward: 0.0, avg loss: 0.00080, eps: 0.599, Time:  8\n",
      "Episode: 105, Reward: 2.0, avg loss: 0.00057, eps: 0.598, Time:  15\n",
      "Episode: 106, Reward: 1.0, avg loss: 0.00069, eps: 0.598, Time:  10\n",
      "Episode: 107, Reward: 0.0, avg loss: 0.00065, eps: 0.598, Time:  9\n",
      "Episode: 108, Reward: 2.0, avg loss: 0.00084, eps: 0.598, Time:  13\n",
      "Episode: 109, Reward: 2.0, avg loss: 0.00067, eps: 0.598, Time:  14\n",
      "Episode: 110, Reward: 2.0, avg loss: 0.00070, eps: 0.597, Time:  15\n",
      "Episode: 111, Reward: 2.0, avg loss: 0.00070, eps: 0.597, Time:  14\n",
      "Episode: 112, Reward: 0.0, avg loss: 0.00080, eps: 0.597, Time:  8\n",
      "Episode: 113, Reward: 1.0, avg loss: 0.00072, eps: 0.597, Time:  11\n",
      "Episode: 114, Reward: 0.0, avg loss: 0.00078, eps: 0.596, Time:  9\n",
      "Episode: 115, Reward: 1.0, avg loss: 0.00058, eps: 0.596, Time:  12\n",
      "Episode: 116, Reward: 1.0, avg loss: 0.00061, eps: 0.596, Time:  12\n",
      "Episode: 117, Reward: 0.0, avg loss: 0.00060, eps: 0.596, Time:  9\n",
      "Episode: 118, Reward: 2.0, avg loss: 0.00067, eps: 0.595, Time:  14\n",
      "Episode: 119, Reward: 0.0, avg loss: 0.00057, eps: 0.595, Time:  9\n",
      "Episode: 120, Reward: 3.0, avg loss: 0.00059, eps: 0.595, Time:  16\n",
      "Episode: 121, Reward: 0.0, avg loss: 0.00045, eps: 0.595, Time:  8\n",
      "Episode: 122, Reward: 0.0, avg loss: 0.00059, eps: 0.595, Time:  8\n",
      "Episode: 123, Reward: 0.0, avg loss: 0.00061, eps: 0.594, Time:  9\n",
      "Episode: 124, Reward: 2.0, avg loss: 0.00056, eps: 0.594, Time:  14\n",
      "Episode: 125, Reward: 0.0, avg loss: 0.00055, eps: 0.594, Time:  8\n",
      "Episode: 126, Reward: 1.0, avg loss: 0.00072, eps: 0.594, Time:  11\n",
      "Episode: 127, Reward: 1.0, avg loss: 0.00081, eps: 0.593, Time:  12\n",
      "Episode: 128, Reward: 3.0, avg loss: 0.00072, eps: 0.593, Time:  18\n",
      "Episode: 129, Reward: 0.0, avg loss: 0.00052, eps: 0.593, Time:  8\n",
      "Episode: 130, Reward: 2.0, avg loss: 0.00063, eps: 0.593, Time:  16\n",
      "Episode: 131, Reward: 1.0, avg loss: 0.00067, eps: 0.592, Time:  12\n",
      "Episode: 132, Reward: 0.0, avg loss: 0.00082, eps: 0.592, Time:  9\n",
      "Episode: 133, Reward: 1.0, avg loss: 0.00080, eps: 0.592, Time:  10\n",
      "Episode: 134, Reward: 0.0, avg loss: 0.00047, eps: 0.592, Time:  8\n",
      "Episode: 135, Reward: 2.0, avg loss: 0.00062, eps: 0.591, Time:  14\n",
      "Episode: 136, Reward: 3.0, avg loss: 0.00064, eps: 0.591, Time:  19\n",
      "Episode: 137, Reward: 0.0, avg loss: 0.00096, eps: 0.591, Time:  8\n",
      "Episode: 138, Reward: 7.0, avg loss: 0.00072, eps: 0.590, Time:  26\n",
      "Episode: 139, Reward: 0.0, avg loss: 0.00080, eps: 0.590, Time:  8\n",
      "Episode: 140, Reward: 0.0, avg loss: 0.00067, eps: 0.590, Time:  9\n",
      "Episode: 141, Reward: 1.0, avg loss: 0.00071, eps: 0.590, Time:  12\n",
      "Episode: 142, Reward: 1.0, avg loss: 0.00077, eps: 0.590, Time:  11\n",
      "Episode: 143, Reward: 1.0, avg loss: 0.00063, eps: 0.589, Time:  11\n",
      "Episode: 144, Reward: 4.0, avg loss: 0.00067, eps: 0.589, Time:  20\n",
      "Episode: 145, Reward: 1.0, avg loss: 0.00071, eps: 0.589, Time:  12\n",
      "Episode: 146, Reward: 2.0, avg loss: 0.00057, eps: 0.588, Time:  11\n",
      "Episode: 147, Reward: 3.0, avg loss: 0.00071, eps: 0.588, Time:  18\n",
      "Episode: 148, Reward: 1.0, avg loss: 0.00075, eps: 0.588, Time:  10\n",
      "Episode: 149, Reward: 2.0, avg loss: 0.00076, eps: 0.588, Time:  13\n",
      "Episode: 150, Reward: 1.0, avg loss: 0.00079, eps: 0.587, Time:  10\n",
      "Episode: 151, Reward: 3.0, avg loss: 0.00072, eps: 0.587, Time:  17\n",
      "Episode: 152, Reward: 2.0, avg loss: 0.00071, eps: 0.587, Time:  15\n",
      "Episode: 153, Reward: 2.0, avg loss: 0.00075, eps: 0.586, Time:  14\n",
      "Episode: 154, Reward: 1.0, avg loss: 0.00067, eps: 0.586, Time:  11\n",
      "Episode: 155, Reward: 2.0, avg loss: 0.00061, eps: 0.586, Time:  15\n",
      "Episode: 156, Reward: 0.0, avg loss: 0.00078, eps: 0.586, Time:  10\n",
      "Episode: 157, Reward: 0.0, avg loss: 0.00088, eps: 0.586, Time:  8\n",
      "Episode: 158, Reward: 2.0, avg loss: 0.00076, eps: 0.585, Time:  16\n",
      "Episode: 159, Reward: 1.0, avg loss: 0.00074, eps: 0.585, Time:  10\n",
      "Episode: 160, Reward: 1.0, avg loss: 0.00035, eps: 0.585, Time:  10\n",
      "Episode: 161, Reward: 1.0, avg loss: 0.00067, eps: 0.585, Time:  10\n",
      "Episode: 162, Reward: 0.0, avg loss: 0.00083, eps: 0.584, Time:  10\n",
      "Episode: 163, Reward: 1.0, avg loss: 0.00067, eps: 0.584, Time:  10\n",
      "Episode: 164, Reward: 1.0, avg loss: 0.00047, eps: 0.584, Time:  10\n",
      "Episode: 165, Reward: 4.0, avg loss: 0.00066, eps: 0.584, Time:  19\n",
      "Episode: 166, Reward: 0.0, avg loss: 0.00084, eps: 0.583, Time:  9\n",
      "Episode: 167, Reward: 2.0, avg loss: 0.00071, eps: 0.583, Time:  14\n",
      "Episode: 168, Reward: 2.0, avg loss: 0.00066, eps: 0.583, Time:  14\n",
      "Episode: 169, Reward: 1.0, avg loss: 0.00052, eps: 0.583, Time:  10\n",
      "Episode: 170, Reward: 1.0, avg loss: 0.00077, eps: 0.582, Time:  12\n",
      "Episode: 171, Reward: 0.0, avg loss: 0.00060, eps: 0.582, Time:  8\n",
      "Episode: 172, Reward: 1.0, avg loss: 0.00062, eps: 0.582, Time:  13\n",
      "Episode: 173, Reward: 1.0, avg loss: 0.00061, eps: 0.582, Time:  14\n",
      "Episode: 174, Reward: 3.0, avg loss: 0.00071, eps: 0.581, Time:  18\n",
      "Episode: 175, Reward: 0.0, avg loss: 0.00046, eps: 0.581, Time:  9\n",
      "Episode: 176, Reward: 2.0, avg loss: 0.00080, eps: 0.581, Time:  14\n",
      "Episode: 177, Reward: 0.0, avg loss: 0.00055, eps: 0.581, Time:  10\n",
      "Episode: 178, Reward: 0.0, avg loss: 0.00066, eps: 0.580, Time:  8\n",
      "Episode: 179, Reward: 0.0, avg loss: 0.00063, eps: 0.580, Time:  9\n",
      "Episode: 180, Reward: 1.0, avg loss: 0.00068, eps: 0.580, Time:  11\n",
      "Episode: 181, Reward: 0.0, avg loss: 0.00064, eps: 0.580, Time:  8\n",
      "Episode: 182, Reward: 0.0, avg loss: 0.00077, eps: 0.580, Time:  9\n",
      "Episode: 183, Reward: 1.0, avg loss: 0.00059, eps: 0.579, Time:  11\n",
      "Episode: 184, Reward: 3.0, avg loss: 0.00061, eps: 0.579, Time:  20\n",
      "Episode: 185, Reward: 0.0, avg loss: 0.00063, eps: 0.579, Time:  8\n",
      "Episode: 186, Reward: 1.0, avg loss: 0.00051, eps: 0.579, Time:  11\n",
      "Episode: 187, Reward: 2.0, avg loss: 0.00067, eps: 0.578, Time:  14\n",
      "Episode: 188, Reward: 3.0, avg loss: 0.00061, eps: 0.578, Time:  16\n",
      "Episode: 189, Reward: 0.0, avg loss: 0.00054, eps: 0.578, Time:  9\n",
      "Episode: 190, Reward: 2.0, avg loss: 0.00094, eps: 0.578, Time:  14\n",
      "Episode: 191, Reward: 4.0, avg loss: 0.00075, eps: 0.577, Time:  22\n",
      "Episode: 192, Reward: 7.0, avg loss: 0.00073, eps: 0.577, Time:  27\n",
      "Episode: 193, Reward: 0.0, avg loss: 0.00063, eps: 0.576, Time:  9\n",
      "Episode: 194, Reward: 3.0, avg loss: 0.00058, eps: 0.576, Time:  16\n",
      "Episode: 195, Reward: 4.0, avg loss: 0.00070, eps: 0.576, Time:  20\n",
      "Episode: 196, Reward: 1.0, avg loss: 0.00086, eps: 0.575, Time:  12\n",
      "Episode: 197, Reward: 2.0, avg loss: 0.00069, eps: 0.575, Time:  14\n",
      "Episode: 198, Reward: 0.0, avg loss: 0.00070, eps: 0.575, Time:  8\n",
      "Episode: 199, Reward: 0.0, avg loss: 0.00058, eps: 0.575, Time:  10\n",
      "Episode: 200, Reward: 4.0, avg loss: 0.00083, eps: 0.574, Time:  21\n",
      "Episode: 201, Reward: 1.0, avg loss: 0.00064, eps: 0.574, Time:  10\n",
      "Episode: 202, Reward: 2.0, avg loss: 0.00082, eps: 0.574, Time:  14\n",
      "Episode: 203, Reward: 0.0, avg loss: 0.00090, eps: 0.574, Time:  9\n",
      "Episode: 204, Reward: 1.0, avg loss: 0.00085, eps: 0.573, Time:  12\n",
      "Episode: 205, Reward: 2.0, avg loss: 0.00076, eps: 0.573, Time:  15\n",
      "Episode: 206, Reward: 3.0, avg loss: 0.00078, eps: 0.573, Time:  17\n",
      "Episode: 207, Reward: 0.0, avg loss: 0.00073, eps: 0.573, Time:  9\n",
      "Episode: 208, Reward: 2.0, avg loss: 0.00066, eps: 0.572, Time:  14\n",
      "Episode: 209, Reward: 1.0, avg loss: 0.00076, eps: 0.572, Time:  12\n",
      "Episode: 210, Reward: 2.0, avg loss: 0.00069, eps: 0.572, Time:  14\n",
      "Episode: 211, Reward: 0.0, avg loss: 0.00066, eps: 0.572, Time:  9\n",
      "Episode: 212, Reward: 5.0, avg loss: 0.00069, eps: 0.571, Time:  21\n",
      "Episode: 213, Reward: 0.0, avg loss: 0.00059, eps: 0.571, Time:  9\n",
      "Episode: 214, Reward: 1.0, avg loss: 0.00085, eps: 0.571, Time:  14\n",
      "Episode: 215, Reward: 3.0, avg loss: 0.00071, eps: 0.570, Time:  17\n",
      "Episode: 216, Reward: 0.0, avg loss: 0.00084, eps: 0.570, Time:  8\n",
      "Episode: 217, Reward: 0.0, avg loss: 0.00051, eps: 0.570, Time:  8\n",
      "Episode: 218, Reward: 5.0, avg loss: 0.00081, eps: 0.570, Time:  21\n",
      "Episode: 219, Reward: 0.0, avg loss: 0.00074, eps: 0.569, Time:  9\n",
      "Episode: 220, Reward: 3.0, avg loss: 0.00066, eps: 0.569, Time:  17\n",
      "Episode: 221, Reward: 2.0, avg loss: 0.00067, eps: 0.569, Time:  14\n",
      "Episode: 222, Reward: 1.0, avg loss: 0.00076, eps: 0.569, Time:  12\n",
      "Episode: 223, Reward: 0.0, avg loss: 0.00071, eps: 0.568, Time:  9\n",
      "Episode: 224, Reward: 0.0, avg loss: 0.00103, eps: 0.568, Time:  9\n",
      "Episode: 225, Reward: 3.0, avg loss: 0.00085, eps: 0.568, Time:  16\n",
      "Episode: 226, Reward: 2.0, avg loss: 0.00079, eps: 0.568, Time:  14\n",
      "Episode: 227, Reward: 1.0, avg loss: 0.00064, eps: 0.567, Time:  12\n",
      "Episode: 228, Reward: 0.0, avg loss: 0.00077, eps: 0.567, Time:  9\n",
      "Episode: 229, Reward: 2.0, avg loss: 0.00051, eps: 0.567, Time:  12\n",
      "Episode: 230, Reward: 3.0, avg loss: 0.00052, eps: 0.567, Time:  18\n",
      "Episode: 231, Reward: 1.0, avg loss: 0.00067, eps: 0.566, Time:  11\n",
      "Episode: 232, Reward: 4.0, avg loss: 0.00068, eps: 0.566, Time:  20\n",
      "Episode: 233, Reward: 0.0, avg loss: 0.00056, eps: 0.566, Time:  9\n",
      "Episode: 234, Reward: 2.0, avg loss: 0.00067, eps: 0.565, Time:  16\n",
      "Episode: 235, Reward: 1.0, avg loss: 0.00058, eps: 0.565, Time:  12\n",
      "Episode: 236, Reward: 1.0, avg loss: 0.00064, eps: 0.565, Time:  12\n",
      "Episode: 237, Reward: 0.0, avg loss: 0.00054, eps: 0.565, Time:  8\n",
      "Episode: 238, Reward: 0.0, avg loss: 0.00058, eps: 0.565, Time:  13\n",
      "Episode: 239, Reward: 0.0, avg loss: 0.00082, eps: 0.564, Time:  8\n",
      "Episode: 240, Reward: 1.0, avg loss: 0.00082, eps: 0.564, Time:  10\n",
      "Episode: 241, Reward: 0.0, avg loss: 0.00074, eps: 0.564, Time:  10\n",
      "Episode: 242, Reward: 3.0, avg loss: 0.00079, eps: 0.564, Time:  16\n",
      "Episode: 243, Reward: 1.0, avg loss: 0.00084, eps: 0.563, Time:  11\n",
      "Episode: 244, Reward: 2.0, avg loss: 0.00077, eps: 0.563, Time:  15\n",
      "Episode: 245, Reward: 0.0, avg loss: 0.00062, eps: 0.563, Time:  9\n",
      "Episode: 246, Reward: 3.0, avg loss: 0.00062, eps: 0.563, Time:  17\n",
      "Episode: 247, Reward: 1.0, avg loss: 0.00068, eps: 0.562, Time:  12\n",
      "Episode: 248, Reward: 1.0, avg loss: 0.00067, eps: 0.562, Time:  10\n",
      "Episode: 249, Reward: 1.0, avg loss: 0.00070, eps: 0.562, Time:  10\n",
      "Episode: 250, Reward: 2.0, avg loss: 0.00075, eps: 0.562, Time:  15\n",
      "Episode: 251, Reward: 0.0, avg loss: 0.00058, eps: 0.561, Time:  9\n",
      "Episode: 252, Reward: 2.0, avg loss: 0.00062, eps: 0.561, Time:  13\n",
      "Episode: 253, Reward: 0.0, avg loss: 0.00052, eps: 0.561, Time:  9\n",
      "Episode: 254, Reward: 2.0, avg loss: 0.00077, eps: 0.561, Time:  13\n",
      "Episode: 255, Reward: 3.0, avg loss: 0.00080, eps: 0.560, Time:  16\n",
      "Episode: 256, Reward: 1.0, avg loss: 0.00063, eps: 0.560, Time:  14\n",
      "Episode: 257, Reward: 0.0, avg loss: 0.00069, eps: 0.560, Time:  8\n",
      "Episode: 258, Reward: 0.0, avg loss: 0.00061, eps: 0.560, Time:  8\n",
      "Episode: 259, Reward: 0.0, avg loss: 0.00074, eps: 0.560, Time:  10\n",
      "Episode: 260, Reward: 0.0, avg loss: 0.00062, eps: 0.559, Time:  9\n",
      "Episode: 261, Reward: 1.0, avg loss: 0.00051, eps: 0.559, Time:  11\n",
      "Episode: 262, Reward: 0.0, avg loss: 0.00094, eps: 0.559, Time:  9\n",
      "Episode: 263, Reward: 2.0, avg loss: 0.00074, eps: 0.559, Time:  13\n",
      "Episode: 264, Reward: 0.0, avg loss: 0.00058, eps: 0.559, Time:  9\n",
      "Episode: 265, Reward: 3.0, avg loss: 0.00068, eps: 0.558, Time:  18\n",
      "Episode: 266, Reward: 2.0, avg loss: 0.00067, eps: 0.558, Time:  16\n",
      "Episode: 267, Reward: 0.0, avg loss: 0.00067, eps: 0.558, Time:  9\n",
      "Episode: 268, Reward: 0.0, avg loss: 0.00054, eps: 0.557, Time:  9\n",
      "Episode: 269, Reward: 1.0, avg loss: 0.00077, eps: 0.557, Time:  12\n",
      "Episode: 270, Reward: 3.0, avg loss: 0.00077, eps: 0.557, Time:  17\n",
      "Episode: 271, Reward: 1.0, avg loss: 0.00067, eps: 0.557, Time:  11\n",
      "Episode: 272, Reward: 1.0, avg loss: 0.00073, eps: 0.556, Time:  10\n",
      "Episode: 273, Reward: 1.0, avg loss: 0.00079, eps: 0.556, Time:  11\n",
      "Episode: 274, Reward: 0.0, avg loss: 0.00065, eps: 0.556, Time:  9\n",
      "Episode: 275, Reward: 3.0, avg loss: 0.00071, eps: 0.556, Time:  19\n",
      "Episode: 276, Reward: 2.0, avg loss: 0.00095, eps: 0.555, Time:  16\n",
      "Episode: 277, Reward: 1.0, avg loss: 0.00091, eps: 0.555, Time:  11\n",
      "Episode: 278, Reward: 1.0, avg loss: 0.00075, eps: 0.555, Time:  13\n",
      "Episode: 279, Reward: 2.0, avg loss: 0.00090, eps: 0.555, Time:  14\n",
      "Episode: 280, Reward: 2.0, avg loss: 0.00058, eps: 0.554, Time:  14\n",
      "Episode: 281, Reward: 0.0, avg loss: 0.00057, eps: 0.554, Time:  9\n",
      "Episode: 282, Reward: 0.0, avg loss: 0.00062, eps: 0.554, Time:  8\n",
      "Episode: 283, Reward: 1.0, avg loss: 0.00067, eps: 0.554, Time:  10\n",
      "Episode: 284, Reward: 2.0, avg loss: 0.00059, eps: 0.553, Time:  13\n",
      "Episode: 285, Reward: 2.0, avg loss: 0.00083, eps: 0.553, Time:  14\n",
      "Episode: 286, Reward: 2.0, avg loss: 0.00071, eps: 0.553, Time:  14\n",
      "Episode: 287, Reward: 1.0, avg loss: 0.00072, eps: 0.553, Time:  11\n",
      "Episode: 288, Reward: 0.0, avg loss: 0.00058, eps: 0.553, Time:  9\n",
      "Episode: 289, Reward: 1.0, avg loss: 0.00075, eps: 0.552, Time:  14\n",
      "Episode: 290, Reward: 0.0, avg loss: 0.00088, eps: 0.552, Time:  9\n",
      "Episode: 291, Reward: 2.0, avg loss: 0.00067, eps: 0.552, Time:  15\n",
      "Episode: 292, Reward: 1.0, avg loss: 0.00046, eps: 0.552, Time:  11\n",
      "Episode: 293, Reward: 0.0, avg loss: 0.00054, eps: 0.551, Time:  9\n",
      "Episode: 294, Reward: 0.0, avg loss: 0.00050, eps: 0.551, Time:  11\n",
      "Episode: 295, Reward: 2.0, avg loss: 0.00060, eps: 0.551, Time:  14\n",
      "Episode: 296, Reward: 2.0, avg loss: 0.00062, eps: 0.551, Time:  13\n",
      "Episode: 297, Reward: 0.0, avg loss: 0.00061, eps: 0.550, Time:  9\n",
      "Episode: 298, Reward: 0.0, avg loss: 0.00059, eps: 0.550, Time:  10\n",
      "Episode: 299, Reward: 3.0, avg loss: 0.00061, eps: 0.550, Time:  18\n",
      "Episode: 300, Reward: 1.0, avg loss: 0.00078, eps: 0.550, Time:  13\n",
      "Episode: 301, Reward: 4.0, avg loss: 0.00072, eps: 0.549, Time:  21\n",
      "Episode: 302, Reward: 3.0, avg loss: 0.00075, eps: 0.549, Time:  16\n",
      "Episode: 303, Reward: 2.0, avg loss: 0.00080, eps: 0.549, Time:  14\n",
      "Episode: 304, Reward: 0.0, avg loss: 0.00062, eps: 0.548, Time:  10\n",
      "Episode: 305, Reward: 3.0, avg loss: 0.00070, eps: 0.548, Time:  16\n",
      "Episode: 306, Reward: 1.0, avg loss: 0.00073, eps: 0.548, Time:  10\n",
      "Episode: 307, Reward: 1.0, avg loss: 0.00086, eps: 0.548, Time:  14\n",
      "Episode: 308, Reward: 0.0, avg loss: 0.00071, eps: 0.547, Time:  8\n",
      "Episode: 309, Reward: 2.0, avg loss: 0.00062, eps: 0.547, Time:  15\n",
      "Episode: 310, Reward: 2.0, avg loss: 0.00080, eps: 0.547, Time:  14\n",
      "Episode: 311, Reward: 1.0, avg loss: 0.00071, eps: 0.547, Time:  13\n",
      "Episode: 312, Reward: 2.0, avg loss: 0.00105, eps: 0.546, Time:  14\n",
      "Episode: 313, Reward: 1.0, avg loss: 0.00068, eps: 0.546, Time:  14\n",
      "Episode: 314, Reward: 1.0, avg loss: 0.00066, eps: 0.546, Time:  12\n",
      "Episode: 315, Reward: 0.0, avg loss: 0.00044, eps: 0.546, Time:  8\n",
      "Episode: 316, Reward: 0.0, avg loss: 0.00079, eps: 0.545, Time:  9\n",
      "Episode: 317, Reward: 3.0, avg loss: 0.00070, eps: 0.545, Time:  15\n",
      "Episode: 318, Reward: 3.0, avg loss: 0.00081, eps: 0.545, Time:  17\n",
      "Episode: 319, Reward: 1.0, avg loss: 0.00078, eps: 0.545, Time:  12\n",
      "Episode: 320, Reward: 1.0, avg loss: 0.00094, eps: 0.544, Time:  11\n",
      "Episode: 321, Reward: 2.0, avg loss: 0.00059, eps: 0.544, Time:  15\n",
      "Episode: 322, Reward: 2.0, avg loss: 0.00057, eps: 0.544, Time:  14\n",
      "Episode: 323, Reward: 3.0, avg loss: 0.00069, eps: 0.543, Time:  17\n",
      "Episode: 324, Reward: 1.0, avg loss: 0.00086, eps: 0.543, Time:  13\n",
      "Episode: 325, Reward: 2.0, avg loss: 0.00066, eps: 0.543, Time:  15\n",
      "Episode: 326, Reward: 1.0, avg loss: 0.00073, eps: 0.543, Time:  12\n",
      "Episode: 327, Reward: 4.0, avg loss: 0.00060, eps: 0.542, Time:  22\n",
      "Episode: 328, Reward: 1.0, avg loss: 0.00065, eps: 0.542, Time:  10\n",
      "Episode: 329, Reward: 0.0, avg loss: 0.00073, eps: 0.542, Time:  8\n",
      "Episode: 330, Reward: 0.0, avg loss: 0.00050, eps: 0.542, Time:  8\n",
      "Episode: 331, Reward: 2.0, avg loss: 0.00070, eps: 0.541, Time:  15\n",
      "Episode: 332, Reward: 1.0, avg loss: 0.00085, eps: 0.541, Time:  12\n",
      "Episode: 333, Reward: 1.0, avg loss: 0.00076, eps: 0.541, Time:  13\n",
      "Episode: 334, Reward: 2.0, avg loss: 0.00080, eps: 0.541, Time:  13\n",
      "Episode: 335, Reward: 1.0, avg loss: 0.00065, eps: 0.540, Time:  12\n",
      "Episode: 336, Reward: 0.0, avg loss: 0.00076, eps: 0.540, Time:  10\n",
      "Episode: 337, Reward: 1.0, avg loss: 0.00065, eps: 0.540, Time:  11\n",
      "Episode: 338, Reward: 2.0, avg loss: 0.00077, eps: 0.540, Time:  16\n",
      "Episode: 339, Reward: 2.0, avg loss: 0.00081, eps: 0.539, Time:  14\n",
      "Episode: 340, Reward: 1.0, avg loss: 0.00067, eps: 0.539, Time:  12\n",
      "Episode: 341, Reward: 1.0, avg loss: 0.00079, eps: 0.539, Time:  11\n",
      "Episode: 342, Reward: 0.0, avg loss: 0.00068, eps: 0.539, Time:  9\n",
      "Episode: 343, Reward: 4.0, avg loss: 0.00060, eps: 0.538, Time:  20\n",
      "Episode: 344, Reward: 1.0, avg loss: 0.00079, eps: 0.538, Time:  12\n",
      "Episode: 345, Reward: 0.0, avg loss: 0.00062, eps: 0.538, Time:  9\n",
      "Episode: 346, Reward: 4.0, avg loss: 0.00075, eps: 0.537, Time:  21\n",
      "Episode: 347, Reward: 1.0, avg loss: 0.00082, eps: 0.537, Time:  13\n",
      "Episode: 348, Reward: 1.0, avg loss: 0.00068, eps: 0.537, Time:  14\n",
      "Episode: 349, Reward: 1.0, avg loss: 0.00074, eps: 0.537, Time:  10\n",
      "Episode: 350, Reward: 1.0, avg loss: 0.00055, eps: 0.536, Time:  12\n",
      "Episode: 351, Reward: 1.0, avg loss: 0.00056, eps: 0.536, Time:  11\n",
      "Episode: 352, Reward: 0.0, avg loss: 0.00084, eps: 0.536, Time:  9\n",
      "Episode: 353, Reward: 1.0, avg loss: 0.00072, eps: 0.536, Time:  13\n",
      "Episode: 354, Reward: 1.0, avg loss: 0.00094, eps: 0.536, Time:  13\n",
      "Episode: 355, Reward: 5.0, avg loss: 0.00074, eps: 0.535, Time:  26\n",
      "Episode: 356, Reward: 0.0, avg loss: 0.00078, eps: 0.535, Time:  10\n",
      "Episode: 357, Reward: 1.0, avg loss: 0.00079, eps: 0.535, Time:  12\n",
      "Episode: 358, Reward: 1.0, avg loss: 0.00067, eps: 0.534, Time:  10\n",
      "Episode: 359, Reward: 0.0, avg loss: 0.00065, eps: 0.534, Time:  8\n",
      "Episode: 360, Reward: 4.0, avg loss: 0.00075, eps: 0.534, Time:  20\n",
      "Episode: 361, Reward: 3.0, avg loss: 0.00065, eps: 0.534, Time:  17\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "    raise SystemError('GPU device not found')\n",
    "with tf.device('/device:GPU:0'):\n",
    "    num_episodes = 10000 #note that they should be even more\n",
    "    eps = MAX_EPSILON\n",
    "    render = False #passing render if you want to render the game during training\n",
    "    train_writer = tf.summary.create_file_writer(STORE_PATH + f\"/DQN_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "\n",
    "    steps = 0\n",
    "    ep_time = current_milli_time()\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = image_preprocess(state)\n",
    "        state_stack = tf.Variable(np.repeat(state.numpy(), NUM_FRAMES).reshape((POST_PROCESS_IMAGE_SIZE[0],\n",
    "                                                                                  POST_PROCESS_IMAGE_SIZE[1],\n",
    "                                                                                  NUM_FRAMES)))\n",
    "        cnt = 1\n",
    "        avg_loss = 0\n",
    "        tot_reward = 0\n",
    "        if i % GIF_RECORDING_FREQ == 0:\n",
    "            frame_list = []\n",
    "        while True:\n",
    "          #if render:\n",
    "          #    env.render()\n",
    "            #eps-greedy action\n",
    "            action = choose_action(state_stack, primary_network, eps, steps)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            tot_reward += reward\n",
    "            if i % GIF_RECORDING_FREQ == 0:\n",
    "                frame_list.append(tf.cast(tf.image.resize(next_state, (480, 320)), tf.uint8).numpy())\n",
    "            next_state = image_preprocess(next_state)\n",
    "            state_stack = process_state_stack(state_stack, next_state)\n",
    "          # store in memory\n",
    "            memory.add_sample(next_state, action, reward, done)\n",
    "\n",
    "            if steps > DELAY_TRAINING:\n",
    "                loss = train(primary_network, memory, target_network)\n",
    "                if i < 100:\n",
    "                    update_network(primary_network, target_network, check= False)\n",
    "                else:\n",
    "                    if i % 100 == 0:\n",
    "                        update_network(primary_network,target_network,check=True)\n",
    "\n",
    "            else:\n",
    "                loss = -1\n",
    "            avg_loss += loss\n",
    "\n",
    "          # linearly decay the eps value\n",
    "            if steps > DELAY_TRAINING:\n",
    "                eps = MAX_EPSILON - ((steps - DELAY_TRAINING) / EPSILON_MIN_ITER) * \\\n",
    "                    (MAX_EPSILON - MIN_EPSILON) if steps < EPSILON_MIN_ITER else \\\n",
    "                  MIN_EPSILON\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                ep_sec_time = int((current_milli_time()-ep_time) / 1000)\n",
    "\n",
    "                if steps > DELAY_TRAINING:\n",
    "                    avg_loss /= cnt\n",
    "                    print(f\"Episode: {i}, Reward: {tot_reward}, avg loss: {avg_loss:.5f}, eps: {eps:.3f}, Time: {ep_sec_time: d}\")\n",
    "                    #tensorboard output\n",
    "                    with train_writer.as_default():\n",
    "                        tf.summary.scalar('reward', tot_reward, step=i)\n",
    "                        tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "                else:\n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                if i % GIF_RECORDING_FREQ == 0:\n",
    "                    record_gif(frame_list, i)\n",
    "                ep_time = current_milli_time()\n",
    "                break\n",
    "\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Actual run in \"Breakout-v0\" environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to source code: \\\n",
    "https://adventuresinmachinelearning.com/atari-space-invaders-dueling-q/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPuGvOo6Dhqg"
   },
   "source": [
    "## Creating the copy section to apply transfer learning on other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "COJvdicFJc8H",
    "outputId": "a3530e7c-6291-4df4-90a9-19be12247d3c"
   },
   "source": [
    "This part it's still work in progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "IY-QJKHgM4ki",
    "outputId": "5c2b3bb1-993c-4f64-80e7-1b04f4a8fd66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-00757cd537dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[0;34m(json_string, custom_objects)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 147\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[0;32m--> 301\u001b[0;31m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    147\u001b[0m                                         list(custom_objects.items())))\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \"\"\"\n\u001b[0;32m-> 1179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/initializers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/initializers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    508\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                                     printable_module_name='initializer')\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 140\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown initializer: GlorotUniform"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import os\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = target_network.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "target_network.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "1WdJkIpNRQgn",
    "outputId": "9ce0c09b-6d6a-436f-c494-9cb470b6e08f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6ded7c363382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./saved_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'save_model' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(\n",
    "    target_network,\n",
    "    \"target_net.h5\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")\n",
    "\n",
    "filepath = './saved_model'\n",
    "save_model(model, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "d6V_28PUERGA",
    "outputId": "8df7026a-29ea-4c9a-8c0a-4f025b0ae61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fe3109dfad02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprimary_net_breakout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprymary_net_breakout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprimary_net_breakout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prymary_net_breakout' is not defined"
     ]
    }
   ],
   "source": [
    "primary_net_breakout = keras.models.load_model(\"SpaceInv_tnet\")\n",
    "primary_net_breakout.layers.pop()\n",
    "\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "num_actions = env.action_space.n\n",
    "space_dim   = env.observation_space.shape #the image input is resized as before\n",
    "\n",
    "for layer in primary_net_breakout.layers:\n",
    "  layer.trainable = False\n",
    "prymary_net_breakout.add(Dense(num_actions))\n",
    "primary_net_breakout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-PSuu0aM4ry"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "QLhGMPPvM4pI",
    "outputId": "6cb95760-a45f-4b76-8951-770808cb30b4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmE9htbhOk8Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "spaceinv_v0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
